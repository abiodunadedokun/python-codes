{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5344403",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [-N N] json\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Copyright (c) Meta Platforms, Inc. and affiliates.\n",
    "\n",
    "This source code is licensed under the MIT license found in the\n",
    "LICENSE file in the root directory of this source tree.\n",
    "\"\"\"\n",
    "import argparse\n",
    "from multiprocessing import Pool\n",
    "import re\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk import edit_distance\n",
    "from tqdm import tqdm\n",
    "\n",
    "import orjson\n",
    "\n",
    "inline_reg = re.compile(r\"\\\\\\((.*?)(?<!\\\\)\\\\\\)\")\n",
    "display_reg = re.compile(r\"\\\\\\[(.+?)(?<!\\\\)\\\\\\]\")\n",
    "table_reg = re.compile(r\"\\\\begin\\{tabular\\}(.+?)(?:\\\\end\\{tabular\\}|$)\", re.S)\n",
    "\n",
    "\n",
    "def compute_metrics(pred, gt, minlen=4):\n",
    "    metrics = {}\n",
    "    if len(pred) < minlen or len(gt) < minlen:\n",
    "        return metrics\n",
    "    metrics[\"edit_dist\"] = edit_distance(pred, gt) / max(len(pred), len(gt))\n",
    "    reference = gt.split()\n",
    "    hypothesis = pred.split()\n",
    "    metrics[\"bleu\"] = nltk.translate.bleu([reference], hypothesis)\n",
    "    try:\n",
    "        metrics[\"meteor\"] = nltk.translate.meteor([reference], hypothesis)\n",
    "    except LookupError:\n",
    "        metrics[\"meteor\"] = np.nan\n",
    "    reference = set(reference)\n",
    "    hypothesis = set(hypothesis)\n",
    "    metrics[\"precision\"] = nltk.scores.precision(reference, hypothesis)\n",
    "    metrics[\"recall\"] = nltk.scores.recall(reference, hypothesis)\n",
    "    metrics[\"f_measure\"] = nltk.scores.f_measure(reference, hypothesis)\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def get_parser():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"json\", type=Path, help=\"results file\")\n",
    "    parser.add_argument(\n",
    "        \"-N\", dest=\"N\", type=int, help=\"number of samples\", default=None\n",
    "    )\n",
    "    args = parser.parse_args()\n",
    "    d = orjson.loads(args.json.read_text(encoding=\"utf-8\"))\n",
    "    args.pred = d[\"predictions\"]\n",
    "    args.gt = d[\"ground_truths\"]\n",
    "    if args.N is not None:\n",
    "        args.pred = args.pred[: args.N]\n",
    "        args.gt = args.gt[: args.N]\n",
    "    return args\n",
    "\n",
    "\n",
    "def split_text(pages: List[str]):\n",
    "    \"\"\"\n",
    "    Split a list of pages into text, inline math, display math, and table blocks.\n",
    "\n",
    "    Args:\n",
    "        pages: The pages to split.\n",
    "    \"\"\"\n",
    "    text, math, table = [], [], []\n",
    "    for page in pages:\n",
    "        for i, reg in enumerate([inline_reg, display_reg, table_reg]):\n",
    "            matches = \"\\n\".join(reg.findall(page))\n",
    "            if i == 2:\n",
    "                table.append(matches)\n",
    "            elif i == 1:\n",
    "                math[-1] += matches\n",
    "            else:\n",
    "                math.append(matches)\n",
    "            page = reg.sub(\"\", page)\n",
    "        text.append(page.strip())\n",
    "\n",
    "    return text, math, table\n",
    "\n",
    "\n",
    "def get_metrics(gt: List[str], pred: List[str], pool: bool = True):\n",
    "    metrics = defaultdict(list)\n",
    "    if pool:\n",
    "        with Pool() as p:\n",
    "            _metrics = p.starmap(compute_metrics, iterable=zip(pred, gt))\n",
    "    else:\n",
    "        _metrics = [compute_metrics(p, g) for p, g in zip(pred, gt)]\n",
    "    for m in _metrics:\n",
    "        for key, value in m.items():\n",
    "            metrics[key].append(value)\n",
    "    return dict(metrics)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    args = get_parser()\n",
    "    for name, entries in zip([\"gt\", \"pred\"], [args.gt, args.pred]):\n",
    "        full: Path = args.json.parent / (args.json.stem + \"_\" + name + \"_full.mmd\")\n",
    "        full.write_text(\"\\n\\n------------------\\n\\n\".join(entries))\n",
    "    for i, (gt, pr) in enumerate(zip(split_text(args.gt), split_text(args.pred))):\n",
    "        sub = [\"Text\", \"Math\", \"Tables\"][i]\n",
    "        prpath: Path = args.json.parent / (\n",
    "            args.json.stem + \"_pred_\" + sub.lower() + \".mmd\"\n",
    "        )\n",
    "        prpath.write_text(\"\\n\\n------------------\\n\\n\".join(pr))\n",
    "        gtpath: Path = args.json.parent / (\n",
    "            args.json.stem + \"_gt_\" + sub.lower() + \".mmd\"\n",
    "        )\n",
    "        gtpath.write_text(\"\\n\\n------------------\\n\\n\".join(gt))\n",
    "        print(\"Results for\", sub)\n",
    "\n",
    "        metrics = get_metrics(gt, pr)\n",
    "        print({key: sum(values) / len(values) for key, values in metrics.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8cdddeeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter ground truth data in LaTeX format:\n",
      "h\n",
      "Enter predictions in LaTeX format:\n",
      "j\n",
      "Metrics for Text:\n",
      "{}\n",
      "\n",
      "Metrics for Math:\n",
      "{}\n",
      "\n",
      "Metrics for Tables:\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk import edit_distance\n",
    "\n",
    "inline_reg = re.compile(r\"\\\\\\((.*?)(?<!\\\\)\\\\\\)\")\n",
    "display_reg = re.compile(r\"\\\\\\[(.+?)(?<!\\\\)\\\\\\]\")\n",
    "table_reg = re.compile(r\"\\\\begin\\{tabular\\}(.+?)(?:\\\\end\\{tabular\\}|$)\", re.S)\n",
    "\n",
    "\n",
    "def compute_metrics(pred, gt, minlen=4):\n",
    "    metrics = {}\n",
    "    if len(pred) < minlen or len(gt) < minlen:\n",
    "        return metrics\n",
    "    metrics[\"edit_dist\"] = edit_distance(pred, gt) / max(len(pred), len(gt))\n",
    "    reference = gt.split()\n",
    "    hypothesis = pred.split()\n",
    "    metrics[\"bleu\"] = nltk.translate.bleu([reference], hypothesis)\n",
    "    try:\n",
    "        metrics[\"meteor\"] = nltk.translate.meteor([reference], hypothesis)\n",
    "    except LookupError:\n",
    "        metrics[\"meteor\"] = np.nan\n",
    "    reference = set(reference)\n",
    "    hypothesis = set(hypothesis)\n",
    "    metrics[\"precision\"] = nltk.scores.precision(reference, hypothesis)\n",
    "    metrics[\"recall\"] = nltk.scores.recall(reference, hypothesis)\n",
    "    metrics[\"f_measure\"] = nltk.scores.f_measure(reference, hypothesis)\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def split_text(pages: List[str]):\n",
    "    text, math, table = [], [], []\n",
    "    for page in pages:\n",
    "        for i, reg in enumerate([inline_reg, display_reg, table_reg]):\n",
    "            matches = \"\\n\".join(reg.findall(page))\n",
    "            if i == 2:\n",
    "                table.append(matches)\n",
    "            elif i == 1:\n",
    "                math[-1] += matches\n",
    "            else:\n",
    "                math.append(matches)\n",
    "            page = reg.sub(\"\", page)\n",
    "        text.append(page.strip())\n",
    "    return text, math, table\n",
    "\n",
    "\n",
    "def get_metrics(gt: List[str], pred: List[str]):\n",
    "    metrics = defaultdict(list)\n",
    "    for m in [compute_metrics(p, g) for p, g in zip(pred, gt)]:\n",
    "        for key, value in m.items():\n",
    "            metrics[key].append(value)\n",
    "    return dict(metrics)\n",
    "\n",
    "\n",
    "def prompt_user_input():\n",
    "    gt = input(\"Enter ground truth data in LaTeX format:\\n\")\n",
    "    pred = input(\"Enter predictions in LaTeX format:\\n\")\n",
    "    return gt, pred\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    gt, pred = prompt_user_input()\n",
    "    gt_pages = gt.strip().split(\"\\n\\n\")\n",
    "    pred_pages = pred.strip().split(\"\\n\\n\")\n",
    "\n",
    "    gt_text, gt_math, gt_table = split_text(gt_pages)\n",
    "    pred_text, pred_math, pred_table = split_text(pred_pages)\n",
    "\n",
    "    metrics_text = get_metrics(gt_text, pred_text)\n",
    "    metrics_math = get_metrics(gt_math, pred_math)\n",
    "    metrics_table = get_metrics(gt_table, pred_table)\n",
    "\n",
    "    print(\"Metrics for Text:\")\n",
    "    print({key: sum(values) / len(values) for key, values in metrics_text.items()})\n",
    "\n",
    "    print(\"\\nMetrics for Math:\")\n",
    "    print({key: sum(values) / len(values) for key, values in metrics_math.items()})\n",
    "\n",
    "    print(\"\\nMetrics for Tables:\")\n",
    "    print({key: sum(values) / len(values) for key, values in metrics_table.items()})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "592d5b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter ground truth data in LaTeX format:\n",
      "C:\\Users\\Acer\\Desktop\\docs\\naggykiss_detected.txt\n",
      "Enter predictions in LaTeX format:\n",
      "C:\\Users\\Acer\\Desktop\\docs\\nagdet.txt\n",
      "Metrics for Text:\n",
      "edit_dist: [0.24489795918367346]\n",
      "bleu: [0]\n",
      "meteor: [0.0]\n",
      "precision: [0.0]\n",
      "recall: [0.0]\n",
      "f_measure: [0]\n",
      "\n",
      "Metrics for Math:\n",
      "\n",
      "Metrics for Tables:\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk import edit_distance\n",
    "\n",
    "# Regular expressions to match LaTeX patterns\n",
    "inline_reg = re.compile(r\"\\\\\\((.*?)(?<!\\\\)\\\\\\)\")\n",
    "display_reg = re.compile(r\"\\\\\\[(.+?)(?<!\\\\)\\\\\\]\")\n",
    "table_reg = re.compile(r\"\\\\begin\\{tabular\\}(.+?)(?:\\\\end\\{tabular\\}|$)\", re.S)\n",
    "\n",
    "\n",
    "def compute_metrics(pred, gt, minlen=4):\n",
    "    metrics = {}\n",
    "    if len(pred) < minlen or len(gt) < minlen:\n",
    "        return metrics\n",
    "    metrics[\"edit_dist\"] = edit_distance(pred, gt) / max(len(pred), len(gt))\n",
    "    reference = gt.split()\n",
    "    hypothesis = pred.split()\n",
    "    metrics[\"bleu\"] = nltk.translate.bleu([reference], hypothesis)\n",
    "    try:\n",
    "        metrics[\"meteor\"] = nltk.translate.meteor([reference], hypothesis)\n",
    "    except LookupError:\n",
    "        metrics[\"meteor\"] = np.nan\n",
    "    reference = set(reference)\n",
    "    hypothesis = set(hypothesis)\n",
    "    metrics[\"precision\"] = nltk.scores.precision(reference, hypothesis)\n",
    "    metrics[\"recall\"] = nltk.scores.recall(reference, hypothesis)\n",
    "    metrics[\"f_measure\"] = nltk.scores.f_measure(reference, hypothesis)\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def split_text(pages: List[str]):\n",
    "    text, math, table = [], [], []\n",
    "    for page in pages:\n",
    "        for i, reg in enumerate([inline_reg, display_reg, table_reg]):\n",
    "            matches = \"\\n\".join(reg.findall(page))\n",
    "            if i == 2:\n",
    "                table.append(matches)\n",
    "            elif i == 1:\n",
    "                math[-1] += matches\n",
    "            else:\n",
    "                math.append(matches)\n",
    "            page = reg.sub(\"\", page)\n",
    "        text.append(page.strip())\n",
    "    return text, math, table\n",
    "\n",
    "\n",
    "def get_metrics(gt: List[str], pred: List[str]):\n",
    "    metrics = defaultdict(list)\n",
    "    for m in [compute_metrics(p, g) for p, g in zip(pred, gt)]:\n",
    "        for key, value in m.items():\n",
    "            metrics[key].append(value)\n",
    "    return dict(metrics)\n",
    "\n",
    "\n",
    "def prompt_user_input():\n",
    "    gt = input(\"Enter ground truth data in LaTeX format:\\n\")\n",
    "    pred = input(\"Enter predictions in LaTeX format:\\n\")\n",
    "    return gt, pred\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    gt, pred = prompt_user_input()\n",
    "    gt_pages = gt.strip().split(\"\\n\\n\")\n",
    "    pred_pages = pred.strip().split(\"\\n\\n\")\n",
    "\n",
    "    gt_text, gt_math, gt_table = split_text(gt_pages)\n",
    "    pred_text, pred_math, pred_table = split_text(pred_pages)\n",
    "\n",
    "    metrics_text = get_metrics(gt_text, pred_text)\n",
    "    metrics_math = get_metrics(gt_math, pred_math)\n",
    "    metrics_table = get_metrics(gt_table, pred_table)\n",
    "\n",
    "    print(\"Metrics for Text:\")\n",
    "    for key, value in metrics_text.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "\n",
    "    print(\"\\nMetrics for Math:\")\n",
    "    for key, value in metrics_math.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "\n",
    "    print(\"\\nMetrics for Tables:\")\n",
    "    for key, value in metrics_table.items():\n",
    "        print(f\"{key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570f9bff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
